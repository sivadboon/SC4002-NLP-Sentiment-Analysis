{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 19967,
     "status": "ok",
     "timestamp": 1731047742046,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "ZCjIpJEDqorT",
    "outputId": "06249251-520b-4d7c-daf0-b75b879baf52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
     ]
    }
   ],
   "source": [
    "# Installing missing libraries\n",
    "!pip install datasets\n",
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y1RzJoztrNU"
   },
   "source": [
    "# Part 0: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VApA04wTeKLR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from datasets import load_dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2001,
     "status": "ok",
     "timestamp": 1731056382065,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "FLBUTTd9uFDe",
    "outputId": "a2a5b492-4a79-48c9-fad0-1f5911fe5aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"], 'label': [1, 1, 1, 1, 1]}\n",
      "{'text': ['compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .', 'the soundtrack alone is worth the price of admission .', 'rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .', \"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\", 'bielinsky is a filmmaker of impressive talent .'], 'label': [1, 1, 1, 1, 1]}\n",
      "{'text': ['lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .', 'consistently clever and suspenseful .', 'it\\'s like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .', 'the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .', 'red dragon \" never cuts corners .'], 'label': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[:5])\n",
    "print(validation_dataset[:5])\n",
    "print(test_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d943ayAGjv2Q"
   },
   "source": [
    "# Part 1. Prepare Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL4VdN9IKaEv"
   },
   "source": [
    "## Answers to Question 1: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1261,
     "status": "ok",
     "timestamp": 1731047812830,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "_R1O3M6RAmQf",
    "outputId": "3c3241f4-4be2-4682-ea64-c93177081c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Tokens: [['the', 'rock', 'is', 'destined', 'to', 'be', 'the', 'st', 'century', 'new', 'conan', 'and', 'that', 'he', 'going', 'to', 'make', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', 'jean', 'claud', 'van', 'damme', 'or', 'steven', 'segal'], ['the', 'gorgeously', 'elaborate', 'continuation', 'of', 'the', 'lord', 'of', 'the', 'rings', 'trilogy', 'is', 'so', 'huge', 'that', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co', 'writer', 'director', 'peter', 'jackson', 'expanded', 'vision', 'of', 'tolkien', 'middle', 'earth'], ['effective', 'but', 'too', 'tepid', 'biopic'], ['if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', 'wasabi', 'is', 'good', 'place', 'to', 'start'], ['emerges', 'as', 'something', 'rare', 'an', 'issue', 'movie', 'that', 'so', 'honest', 'and', 'keenly', 'observed', 'that', 'it', 'doesn', 'feel', 'like', 'one']]\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    return simple_preprocess(text, deacc=True)  # deacc = True -> remove accent marks e.g. café to cafe\n",
    "\n",
    "# Preprocess the training dataset\n",
    "train_tokens = [preprocess_text(example['text']) for example in train_dataset]\n",
    "\n",
    "# Print the first 5 preprocessed sentences\n",
    "print(\"First 5 Tokens:\", train_tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD5PGbqfgEbN"
   },
   "source": [
    "### 1(a): What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1731047819895,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "qZehmAwtFatD",
    "outputId": "c9f7b5da-9143-4be4-bfae-bef7156c7fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 16256\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the train dataset tokens and get the unique words to build vocabulary\n",
    "vocab_counter = Counter([word for tokens in train_tokens for word in tokens])\n",
    "train_vocab = list(vocab_counter.keys())\n",
    "\n",
    "# Get the size of the vocabulary\n",
    "vocab_size = len(train_vocab)\n",
    "\n",
    "# Answer to 1(a)\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOshRHrLgPeR"
   },
   "source": [
    "### 1(b): How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ep7WspGIgpRv"
   },
   "source": [
    "There are mulitple pre-trained models in the Gensim library in Python:\n",
    "- ```conceptnet-numberbatch-17-06-300```\n",
    "- ```fasttext-wiki-news-subwords-300```\n",
    "- ```glove-twitter-25```\n",
    "- ```glove-twitter-50```\n",
    "- ```glove-twitter-100```\n",
    "- ```glove-twitter-200```\n",
    "- ```glove-wiki-gigaword-50```\n",
    "- ```glove-wiki-gigaword-100```\n",
    "- ```glove-wiki-gigaword-200```\n",
    "- ```glove-wiki-gigaword-300```\n",
    "- ```word2vec-google-news-300```\n",
    "- ```word2vec-ruscorpora-300```\n",
    "\n",
    "where the number in ```model-name-number``` represents the dimensionality of the word vectors\n",
    "- Lower-dimensional embeddings (like 25 or 50) are faster to compute and use less memory, but they might capture less nuance in word meanings.\n",
    "- Higher-dimensional embeddings (like 100 or 200) capture more detailed relationships between words, which can improve model performance for NLP tasks but require more memory and computational power.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqRN4Miqi4K0"
   },
   "source": [
    "We will only analyse the OOVs for ```word2vec-google-news-300```, ```glove-twitter``` and ```glove-wiki-gigaword``` as rotten tomatoes data contain more informal and social media terms which makes it more relevant to provide more similar vocabulary.\n",
    "\n",
    "The benchmark for dimensionality will start at **100** before comparing to higher dimensionality models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFx-tT-kqJ97"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import os\n",
    "\n",
    "# List of model names to download and save\n",
    "target_models = [\n",
    "    \"word2vec-google-news-300\",\n",
    "    \"glove-twitter-100\",\n",
    "    \"glove-twitter-200\",\n",
    "    \"glove-wiki-gigaword-100\",\n",
    "    \"glove-wiki-gigaword-200\",\n",
    "    \"glove-wiki-gigaword-300\"\n",
    "]\n",
    "\n",
    "# File Path to directory to store the pre-defined models (change as necessary)\n",
    "model_dir_path = \"/content/drive/My Drive/SC4002/Models\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(model_dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-3zaT1PfdXT"
   },
   "outputs": [],
   "source": [
    "# Loop through each model to download and save from gensim library (will take a few minutes)\n",
    "for model_name in target_models:\n",
    "    print(f\"Downloading and saving model: {model_name}\")\n",
    "    model = api.load(model_name)\n",
    "    model_file = os.path.join(model_dir_path, f\"{model_name}.model\")\n",
    "    model.save(model_file)\n",
    "    print(f\"Model saved to {model_file}\")\n",
    "\n",
    "print(\"All models downloaded and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6643,
     "status": "ok",
     "timestamp": 1731056701596,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "aAyfE6jY2yfP",
    "outputId": "b307b30a-b447-4e6d-8867-390353a713df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model: word2vec-google-news-300\n",
      "Size of vocabulary in word2vec-google-news-300: 3000000\n",
      "Number of OOV words for word2vec-google-news-300: 1454\n",
      "\n",
      "Loading model: glove-twitter-100\n",
      "Size of vocabulary in glove-twitter-100: 1193514\n",
      "Number of OOV words for glove-twitter-100: 1477\n",
      "\n",
      "Loading model: glove-twitter-200\n",
      "Size of vocabulary in glove-twitter-200: 1193514\n",
      "Number of OOV words for glove-twitter-200: 1477\n",
      "\n",
      "Loading model: glove-wiki-gigaword-100\n",
      "Size of vocabulary in glove-wiki-gigaword-100: 400000\n",
      "Number of OOV words for glove-wiki-gigaword-100: 546\n",
      "\n",
      "Loading model: glove-wiki-gigaword-200\n",
      "Size of vocabulary in glove-wiki-gigaword-200: 400000\n",
      "Number of OOV words for glove-wiki-gigaword-200: 546\n",
      "\n",
      "Loading model: glove-wiki-gigaword-300\n",
      "Size of vocabulary in glove-wiki-gigaword-300: 400000\n",
      "Number of OOV words for glove-wiki-gigaword-300: 546\n",
      "\n",
      "######## Summary of OOV counts per model ########\n",
      "word2vec-google-news-300: 1454 OOV words\n",
      "glove-twitter-100: 1477 OOV words\n",
      "glove-twitter-200: 1477 OOV words\n",
      "glove-wiki-gigaword-100: 546 OOV words\n",
      "glove-wiki-gigaword-200: 546 OOV words\n",
      "glove-wiki-gigaword-300: 546 OOV words\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store OOV counts for each model\n",
    "oov_counts = {}\n",
    "\n",
    "# Loop through each model, load it, and calculate OOV\n",
    "for model_name in target_models:\n",
    "\n",
    "    # Load the model from the saved file\n",
    "    model_path = os.path.join(model_dir_path, f\"{model_name}.model\")\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    model = KeyedVectors.load(model_path, mmap='r')\n",
    "\n",
    "    # Get the vocabulary size of the model\n",
    "    model_vocab_size = len(model.key_to_index)\n",
    "    print(f\"Size of vocabulary in {model_name}:\", model_vocab_size)\n",
    "\n",
    "    # Calculate OOV words for the model by comparing with the words in train_vocab\n",
    "    oov_words = [word for word in train_vocab if word not in model.key_to_index]\n",
    "    oov_count = len(oov_words)\n",
    "    print(f\"Number of OOV words for {model_name}:\", oov_count)\n",
    "\n",
    "    # Store the result\n",
    "    oov_counts[model_name] = oov_count\n",
    "\n",
    "# Print summary of OOV counts for each model\n",
    "print(\"\\n######## Summary of OOV counts per model ########\")\n",
    "for model_name, count in oov_counts.items():\n",
    "    print(f\"{model_name}: {count} OOV words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKMoqXDQ5Mfz"
   },
   "source": [
    "From the summary result above, we can deduce that the number of OOV words is independent of the dimensionality of the model.\n",
    "> The reason for this is because GloVe embeddings are pre-trained word vectors that come with a fixed vocabulary based on the corpus they were trained on.\n",
    "\n",
    "Hence, we will proceed to use ```glove-wiki-gigaword-300``` as it has the least number of OOV words out of the 3 models and it has a good balance of high dimensionality and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fPloQYl7Txk"
   },
   "source": [
    "### 1(c): Implement a strategy to solve the OOV problems present in Word2vec (or Glove)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc9jCzso85e-"
   },
   "source": [
    "### First Strategy: Use Lemmatization with Fallback Embedding\n",
    "- Lemmatization works by reducing the words to their base or dictionary form, i.e. running, ran, runs -> run\n",
    "- Then, for the words that are still not found or rare, create an embedded matrix that assigns the average vector of the vocabulary to these words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1731047918168,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "bu5R9WJHGNjM",
    "outputId": "c7e56720-1668-4ba4-abf5-d674d062b367"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet lemmatizer data\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess a word with lemmatization\n",
    "def preprocess_word(word):\n",
    "    lemmatized_word = lemmatizer.lemmatize(word.lower())  # Convert to lowercase and lemmatize\n",
    "    return lemmatized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6169,
     "status": "ok",
     "timestamp": 1731047925865,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "rXo5-leVCfx0",
    "outputId": "2af1dc90-d385-467d-96df-a80c8b8eb7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words (without Lemmatization and Fallback Embedding): 546\n",
      "Number of OOV words (with Lemmatization and Fallback Embedding): 538\n"
     ]
    }
   ],
   "source": [
    "# Load glove-wiki-gigaword-200 model\n",
    "glove_model = KeyedVectors.load(os.path.join(model_dir_path, \"glove-wiki-gigaword-200.model\"))\n",
    "embedding_dim = glove_model.vector_size\n",
    "\n",
    "# Calculate the average vector for fallback\n",
    "avg_vector = np.mean([glove_model[word] for word in glove_model.key_to_index], axis=0)\n",
    "\n",
    "# Function to get GloVe embeddings with fallback handling\n",
    "def get_glove_embedding(word):\n",
    "    word = preprocess_word(word)\n",
    "    if word in glove_model.key_to_index:\n",
    "        return glove_model[word]\n",
    "    else:\n",
    "        return avg_vector  # Use average vector as a fallback for OOV words\n",
    "\n",
    "# OOV Count without Lemmatization and Fallback Embedding\n",
    "oov_count_before = sum(1 for word in train_vocab if word not in glove_model.key_to_index)\n",
    "print(\"Number of OOV words (without Lemmatization and Fallback Embedding):\", oov_count_before)\n",
    "\n",
    "# OOV Count with Lemmatization and Fallback Embedding\n",
    "oov_count_after = sum(1 for word in train_vocab if preprocess_word(word) not in glove_model.key_to_index)\n",
    "print(\"Number of OOV words (with Lemmatization and Fallback Embedding):\", oov_count_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-sCHBseGnts"
   },
   "source": [
    "### Second Strategy: FastText with Subword Embeddings\n",
    "\n",
    "FastText’s subword embeddings reduce OOV words by:\n",
    "- Generating embeddings for any word through character n-grams, even if the full word does not appear in the training data\n",
    "- Producing meaningful representations for morphologically rich words, incorrect spellings, and unknown terms by using common subword patterns\n",
    "- Eliminating the need for fallback strategies, unlike GloVe, which requires preprocessing or fallback vectors to handle OOV words\n",
    "\n",
    "FastText has the following two models:\n",
    "- [```wiki.en.vec```](https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec)\n",
    "  - Model was trained exclusively on English Wikipedia data\n",
    "  - The vocabulary is focused on formal, encyclopedic language, which includes terms from diverse domains like history, science, arts, and popular culture\n",
    "  - Typically smaller than Common Crawl models since Wikipedia has a limited (though diverse) lexicon, centered around factual, standardised language\n",
    "- [```cc.en.300.vec```](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz)\n",
    "  - Model was trained on the Common Crawl dataset, a massive, multilingual dataset pulled from a wide range of online sources\n",
    "  - The vocabulary is larger and more diverse. It captures a broader array of language, including slang, informal speech, niche terminology, and multilingual content\n",
    "  - Significantly larger due to the vast range of sources, making it suitable for general NLP applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFw4eTWLsvCe"
   },
   "outputs": [],
   "source": [
    "# Note: These models will take a few minutes to run\n",
    "# Load the Wiki FastText vectors from the .vec file\n",
    "wiki_fasttext_model = KeyedVectors.load_word2vec_format(os.path.join(model_dir_path, \"wiki.en.vec\"), binary=False)\n",
    "\n",
    "# Load the Common Crawl FastText vectors from the .vec file\n",
    "crawl_fasttext_model = KeyedVectors.load_word2vec_format(os.path.join(model_dir_path, \"cc.en.300.vec\"), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1731052675317,
     "user": {
      "displayName": "Nathanael Lye",
      "userId": "07135139287447984914"
     },
     "user_tz": -480
    },
    "id": "rkWzcSJ7J7Fh",
    "outputId": "f801c104-675f-4f38-d941-25a363638431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words (Wikipedia Model): 240\n",
      "Number of OOV words (Common Crawl Model): 734\n"
     ]
    }
   ],
   "source": [
    "# Function to get FastText embeddings, with subword handling\n",
    "def get_fasttext_embedding(model, word):\n",
    "    return model[word]\n",
    "\n",
    "# Initialize OOV counters\n",
    "wiki_oov_count = 0\n",
    "crawl_oov_count = 0\n",
    "\n",
    "# Loop through each word in the vocabulary\n",
    "for word in train_vocab:\n",
    "    preprocessed_word = preprocess_word(word)\n",
    "\n",
    "    # Check if the word is in the vocabulary of the Wikipedia model\n",
    "    if preprocessed_word not in wiki_fasttext_model:\n",
    "        wiki_oov_count += 1\n",
    "    else:\n",
    "        # Retrieve embedding if the word is in the vocabulary\n",
    "        wiki_embedding = get_fasttext_embedding(wiki_fasttext_model, preprocessed_word)\n",
    "\n",
    "    # Check if the word is in the vocabulary of the Common Crawl model\n",
    "    if preprocessed_word not in crawl_fasttext_model:\n",
    "        crawl_oov_count += 1\n",
    "    else:\n",
    "        # Retrieve embedding if the word is in the vocabulary\n",
    "        crawl_embedding = get_fasttext_embedding(crawl_fasttext_model, preprocessed_word)\n",
    "\n",
    "# Print the number of OOV words for each model\n",
    "print(\"Number of OOV words (Wikipedia Model):\", wiki_oov_count)\n",
    "print(\"Number of OOV words (Common Crawl Model):\", crawl_oov_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bdUZenVx-MW"
   },
   "source": [
    "From the result of the number of OOV words from the two models, we can see that ```wiki.en.vec``` model performed better which was not expected as the Rotten Tomatoes reviews are more conversational and could be thought to contain slang or informal language.\n",
    "\n",
    "Some possible reasons could be due to:\n",
    "- **Relevant Vocabulary:** Wikipedia includes extensive formal descriptions and movie-related vocabulary that aligns with the language used in reviews.\n",
    "- **Reduced Informal Noise:** ```cc.en.300.vec``` covers broader, more informal internet language, leading to mismatches with the structured, descriptive tone of movie reviews.\n",
    "- **Coverage of Proper Nouns and Critic Terms:** Wikipedia’s curated content better captures names, technical terms, and critic jargon, reducing OOV rates for datasets centered on entertainment media.\n",
    "\n",
    "Hence, we will use ```wiki.en.vec``` embedded matrix as the method to reduce the number of OOV words present."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMyLctpIVs6vMFJCGrhG+83",
   "gpuType": "T4",
   "mount_file_id": "1iy9Gd_Vgkz7gR9mryFphQfQRyzDzCVwd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
